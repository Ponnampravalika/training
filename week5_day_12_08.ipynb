{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac32c2cf-dde1-4536-88dd-9d1e93c89f9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------+\n|customer_id|         name|region|\n+-----------+-------------+------+\n|          1|Alice Johnson|  West|\n|          3|  Charlie Lee|  West|\n+-----------+-------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "spark = SparkSession.builder.appName(\"CustomerData\").getOrCreate()\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS retail.customer_data\")\n",
    "raw_schema = \"\"\"\n",
    "    customer_id STRING,\n",
    "    name STRING,\n",
    "    region STRING,\n",
    "    status STRING,\n",
    "    signup_date STRING\n",
    "\"\"\"\n",
    "customer_data = [\n",
    "    (\"1\", \"Alice Johnson\", \"West\", \"Active\", \"2023-01-15\"),\n",
    "    (\"2\", \"Bob Smith\", \"East\", \"Inactive\", \"2023-02-20\"),\n",
    "    (\"3\", \"Charlie Lee\", \"West\", \"Active\", \"2023-03-10\")\n",
    "]\n",
    "customers_df = spark.createDataFrame(customer_data, schema=raw_schema) \\\n",
    "    .withColumn(\"customer_id\", col(\"customer_id\").cast(\"int\")) \\\n",
    "    .withColumn(\"signup_date\", col(\"signup_date\").cast(\"date\"))\n",
    "customers_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"retail.customer_data.customers\")\n",
    "result_df = spark.sql(\"\"\"\n",
    "    SELECT customer_id, name, region\n",
    "    FROM retail.customer_data.customers\n",
    "    WHERE status = 'Active' AND region = 'West'\n",
    "\"\"\")\n",
    "result_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db97acf1-a006-4917-95be-afc74d7a46de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----+---------+\n|product_id|      name|stock|warehouse|\n+----------+----------+-----+---------+\n|       101|    Laptop|   45|        A|\n|       102|Smartphone|  100|        B|\n|       103|    Tablet|   30|        A|\n+----------+----------+-----+---------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "spark = SparkSession.builder.appName(\"InventoryUpdates\").getOrCreate()\n",
    "\n",
    "# Stub for creating the schema in Unity Catalog (assume catalog 'ecommerce' exists)\n",
    "spark.sql(\"CREATE CATALOG IF NOT EXISTS ecommerce\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS ecommerce.inventory\")\n",
    "\n",
    "# Define schema for inventory\n",
    "inventory_schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"stock\", IntegerType(), True),\n",
    "    StructField(\"warehouse\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Initial inventory data\n",
    "initial_data = [\n",
    "    (101, \"Laptop\", 50, \"A\"),\n",
    "    (102, \"Smartphone\", 100, \"B\")\n",
    "]\n",
    "initial_df = spark.createDataFrame(initial_data, schema=inventory_schema)\n",
    "\n",
    "# Write initial data as Delta table\n",
    "initial_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"ecommerce.inventory.products\")\n",
    "\n",
    "# Update batch data\n",
    "update_data = [\n",
    "    (101, \"Laptop\", 45, \"A\"),\n",
    "    (103, \"Tablet\", 30, \"A\")\n",
    "]\n",
    "update_df = spark.createDataFrame(update_data, schema=inventory_schema)\n",
    "\n",
    "\n",
    "# TODO: Perform a merge (upsert) into the Delta table\n",
    "# Hint: Use DeltaTable.forName(spark, \"ecommerce.inventory.products\").alias(\"target\")\n",
    "# Then .merge(update_df.alias(\"source\"), \"target.product_id = source.product_id\")\n",
    "# .whenMatchedUpdate(set={\"stock\": \"source.stock\"})\n",
    "# .whenNotMatchedInsert(values={\"product_id\": \"source.product_id\", \"name\": \"source.name\", \"stock\": \"source.stock\", \"warehouse\": \"source.warehouse\"})\n",
    "# .execute()\n",
    "\n",
    "# TODO: Query the final inventory\n",
    "# Hint: spark.sql(\"SELECT * FROM ecommerce.inventory.products ORDER BY product_id\")\n",
    "delta_table = DeltaTable.forName(spark, \"ecommerce.inventory.products\")\n",
    "\n",
    "(\n",
    "    delta_table.alias(\"target\")\n",
    "    .merge(\n",
    "        update_df.alias(\"source\"),\n",
    "        \"target.product_id = source.product_id\"\n",
    "    )\n",
    "    .whenMatchedUpdate(set={\n",
    "        \"stock\": \"source.stock\"\n",
    "    })\n",
    "    .whenNotMatchedInsert(values={\n",
    "        \"product_id\": \"source.product_id\",\n",
    "        \"name\": \"source.name\",\n",
    "        \"stock\": \"source.stock\",\n",
    "        \"warehouse\": \"source.warehouse\"\n",
    "    })\n",
    "    .execute()\n",
    ")\n",
    "\n",
    "final_df = spark.sql(\"\"\"\n",
    "    SELECT * \n",
    "    FROM ecommerce.inventory.products \n",
    "    ORDER BY product_id\n",
    "\"\"\")\n",
    "final_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bac848f-71f5-422d-9f5d-afe0f43146ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n|total_amount|\n+------------+\n|       25000|\n+------------+\n\n+------------+\n|total_amount|\n+------------+\n|       27000|\n+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SalesAnalysis\").getOrCreate()\n",
    "\n",
    "# Stub for creating the schema in Unity Catalog (assume catalog 'finance' exists)\n",
    "spark.sql(\"CREATE CATALOG IF NOT EXISTS finance\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS finance.sales\")\n",
    "\n",
    "# Define schema for sales\n",
    "sales_schema = StructType([\n",
    "    StructField(\"sale_id\", IntegerType(), False),\n",
    "    StructField(\"product\", StringType(), True),\n",
    "    StructField(\"amount\", IntegerType(), True),\n",
    "    StructField(\"quarter\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Initial sales data\n",
    "initial_data = [\n",
    "    (1, \"Stocks\", 10000, \"Q1-2023\"),\n",
    "    (2, \"Bonds\", 15000, \"Q1-2023\")\n",
    "]\n",
    "initial_df = spark.createDataFrame(initial_data, schema=sales_schema)\n",
    "\n",
    "# Write initial data as Delta table with history enabled\n",
    "initial_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"finance.sales.quarterly_sales\")\n",
    "\n",
    "# Update data (correction)\n",
    "update_data = [\n",
    "    (1, \"Stocks\", 12000, \"Q1-2023\")\n",
    "]\n",
    "update_df = spark.createDataFrame(update_data, schema=sales_schema)\n",
    "\n",
    "# Perform update\n",
    "delta_table = DeltaTable.forName(spark, \"finance.sales.quarterly_sales\")\n",
    "delta_table.alias(\"target\").merge(\n",
    "    update_df.alias(\"source\"),\n",
    "    \"target.sale_id = source.sale_id\"\n",
    ").whenMatchedUpdate(set={\"amount\": \"source.amount\"}).execute()\n",
    "\n",
    "# TODO: Query total amount from previous version using time travel\n",
    "# Hint: First, get the previous version number with delta_table.history().select(\"version\").collect()[1][0] (assuming version 0 is initial)\n",
    "# Then, spark.sql(f\"SELECT SUM(amount) AS total_amount FROM finance.sales.quarterly_sales VERSION AS OF <previous_version>\")\n",
    "\n",
    "# TODO: Query total amount from current version\n",
    "# Hint: spark.sql(\"SELECT SUM(amount) AS total_amount FROM finance.sales.qua\n",
    "history_df = delta_table.history()\n",
    "previous_version = history_df.select(\"version\").collect()[1][0]\n",
    "spark.sql(f\"SELECT SUM(amount) AS total_amount FROM finance.sales.quarterly_sales VERSION AS OF {previous_version}\").show()\n",
    "\n",
    "# Current version total\n",
    "spark.sql(f\"SELECT SUM(amount) AS total_amount FROM finance.sales.quarterly_sales\").show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "week5_day_12_08",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}